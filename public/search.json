[{"categories":null,"content":"Quick demo Features Write plugins in yaml Specify automation process in yaml, such as mouse, keyboard, web, visual (e.g., image recognition), and many other types of actions.\n1 2 3 4 actions: # click a location in target window - window.is(System Settings): - mouse.click({{ [0,200] }}) Simple GUI for plugins using JSON You can write a simple plugin with GUI for your favorite command line tools or REST APIs, in just a few lines of code.\n1 2 3 4 5 6 7 8 9 10 actions: # This will trigger a popup window to ask for inputs - \u003e- user.input({{ { 'title': 'Input your name' 'content': { 'type': 'text', 'label': 'Enter your name' }, } }}) =\u003e $resp Screen annotations Inside plugins, you can use screen annotations to highlight a region, or draw a shape on the screen, to guide users to use the software.\n1 2 3 4 5 6 7 8 9 10 actions: - window.is(System Settings): - \u003e- window.annotate({{ [ { 'type': 'box', 'position': [0, 0], 'size': [100, 100] } ] }}) Connect apps into a pipeline You can connect multiple apps into a pipeline, and control the flow of the pipeline using a simple GUI.\n1 2 3 4 5 actions: # send a discord message when new email arrives - event.on(__IMAP__, {{ {'email':'...', 'key':'...'} }}) =\u003e $r: - data.read( {{ $r['title'] }}) =\u003e $title - web.http(POST, https://discord.com/api/webhooks/...) GUI components Elements Text input or display-only text label 1 2 3 4 5 6 7 { 'type': 'text', 'label': 'Enter your name', # it is display-only if `key` is missing 'key': 'name' } Select from a list of static options 1 2 3 4 5 6 7 8 9 10 { 'type': 'select', 'label': 'Select your favorite options', 'options': ['option1', 'option2', 'option3'], # it is display-only if `key` is missing 'key': 'selected' 'max': 5, 'min': 1 } Select from a list of dynamic options. This can be useful when you want to search local disk, call REST APIs, or run a command to get a list of options dynamically. 1 2 3 4 5 6 7 8 9 10 11 { 'type': 'dynamic', 'label': 'Select your favorite options', # this can be a URL or a command that returns a list 'source': 'wss://localhost:5678/files', 'params': { 'path': '/home/user/desktop' 'callback': 'https://localhost:5678/' }, } Layouts List: arrange elements in vertical order 1 2 3 4 5 6 { 'type': 'list', 'content': [ {...}, {...} ] } Tabs: arrange elements in different pages/tabs 1 2 3 4 5 6 { 'type': 'tabs', 'content': [ {...}, {...} ] } Why did I write this tool? Make it easier to accommodate to some custom automation needs, and avoid installing so many apps just for a simple task. Especially when some apps are heavyweight but you only need a small feature in it.\nGive user control to define and adjust the GUI according to their own needs. You can easily build a handy and minimal GUI tool to call your favorite command line tools or REST APIs.\nProvide a structured way to define GUI and automation, so that it can be easily learned by LLMs, and used to generate automation pipelines with full human control.\nWhat’s next? IR and program generation for robots, something like GPT for robotics. The basic idea is to provide an ISA like interface for robots, and use GPT to generate programs for robots. demo: talk and control a GPT-powered robot. ","description":"","tags":["plugin system","DSL","automation"],"title":"AuTool: porting browser extension to desktop","uri":"/posts/23-05-03-desktop-plugin-system-design/"},{"categories":null,"content":"Misc Extending Slapo to distributed inference might not be a good idea. Slapo’s key feature is “trace-by-need” progressive optimization, which is not useful in inference.\nSlapo only has DeepSpeed and Megatron-LM backends for now. DeepSpeed is less optimized than FT on inference speed.\nFramework overhead is large: from Nvidia Nsight’s profiling report, the kernel launch overhead in DeepSpeed is way higher (many more kernel calls and more kernel counts). Maybe we can consider using CUDA Graph context manager to reduce overhead.\nKernel fusion: can we use Slapo .replace() primitive to replace kernels, say AI template or other hand-tuned CUDA kernels?\nCompiler approach for kernel fusion to improve usability (i.e., generating or replacing fused kernel)?\nSparse in PyTorch STen: Productive and Efficient Sparsity in PyTorch: customizable sparse tensor layout in PyTorch (idea is similar to TACO or TVM SpareTIR) for inference + training. Source code\nExample of specifying custom sparse layout: CSC. Kernel is not auto-generated. May fallback to dense kernel.\n1 2 3 4 5 6 7 8 9 10 11 12 # Define custom sparse layout class CscTensor: def __init__(self, data): self.data = scipy.sparse.csc_matrix(data) def to_dense(self): return torch.from_numpy(self.data.todense()) # Use PM to dispatch CSR*dense to target implementation a = sten.torch_tensor_to_csr(sparsifier, torch.randn(4, 4)) b = torch.randn(4, 4) c = torch.mm(a, b) Case study of n:m sparse format.\nCan we build something upon STen to support kernel auto-generation and other sparse formats? 2:4 in Ampere/cuSPARSE, big bird and such for attention layers.\nTACO: auto-generate sparse kernels with scheduling language. Source: https://github.com/tensor-compiler/taco Limited types of sparse formats are supported 1 2 Format csr({Dense,Sparse}); Tensor\u003cdouble\u003e A({2,3}, csr); DGL sparse. xtransformer. block sparsity. usability++ and support more sparse formats for decoder inference. ","description":"","tags":["transformers","sparsity","distributed inference"],"title":"Exploration on Spare Attention","uri":"/posts/23-04-25-spare-attention/"},{"categories":null,"content":"Sparse kernel generation in Sten ","description":"","tags":["transformers","sparsity","distributed inference"],"title":"Proposal: STen with automatic sparse kernel generation","uri":"/posts/23-05-03-proposal-sten-for-decoder/"},{"categories":null,"content":"Install FT natively TL;DR: I cannot successfully run the FT benchmarking with native installation. Kept on running into this error: https://github.com/NVIDIA/FasterTransformer/issues/177\nEnvironment: A6000 (Ampere, SM86, 48GB memory), CUDA 11.7, PyTorch (1.13), g++ (Ubuntu 9.4.0-1ubuntu1~20.04.1) 9.4.0\nBuild from source.\n1 2 3 4 5 6 $ export INSTALL_PATH=/work/zhang-capra/users/sx233/install # Build with PyTorch support on $ cmake -DSM=86 -DCMAKE_BUILD_TYPE=Release -DBUILD_PYT=ON -DBUILD_MULTI_GPU=ON -DCMAKE_PREFIX_PATH=$INSTALL_PATH .. $ git submodule init \u0026\u0026 git submodule update $ make -j32 Additional dependencies: huggingface transformers, PyO3, Rust nightly (required for PyO3 binding) 1 2 3 4 5 # A nightly build of Rust is required for PyO3 # https://github.com/huggingface/transformers/issues/2831#issuecomment-600141935 $ curl --proto '=https' --tlsv1.2 -sSf https://sh.rustup.rs | sh $ source $HOME/.cargo/env $ pip install transformers==2.5.1 Changes made to compile FT As there is no nvidia-docker installed on the server, I installed the whole thing in a native way.\nDO NOT use conda to install pytroch \u0026 MKL. It leads to lots of undefined reference errors to MKL APIs\n1 pip install torch==1.13.1+cu117 torchvision==0.14.1+cu117 --extra-index-url https://download.pytorch.org/whl/cu117 Patch: link mpi_utils with -lmpi_cxx.To fix the error undefined reference to MPI::Datatype::Datatype() in linking process 1 2 3 if (BUILD_MULTI_GPU) target_link_libraries(mpi_utils PUBLIC -lmpi -lmpi_cxx logger) endif() Patch: install PyTorch from source with MPI support. Otherwise, torch.distributed is not available. https://discuss.pytorch.org/t/distributed-pytorch-with-mpi/77106/4 1 2 3 $ python ../examples/pytorch/bert/bert_example.py 1 12 32 12 64 --data_type fp16 --time [INFO] MPI is not available in this PyTorch build. [FT][WARNING] Skip NCCL initialization since requested tensor/pipeline parallel sizes are equals to 1. Install FT with Nvidia docker No enough space on zhang-x3: Environment: 2080Ti x8 (Turing, 11GB DDR6), Ubuntu 20.04 (g++ 9.4.0), CMake 3.16. 1 docker: write /var/lib/docker/tmp/GetImageBlob677064158: no space left on device. Private work station (3060Ti x2, 3070 x2, Ampere, 8GB GDDR6, SM86): Ubuntu 22.04 (g++ 11.3.0), CUDA 11.5 1 2 3 4 5 6 7 8 9 10 11 12 13 # GPG key adding to apt-key distribution=$(. /etc/os-release;echo $ID$VERSION_ID) \\ \u0026\u0026 curl -fsSL https://nvidia.github.io/libnvidia-container/gpgkey | sudo gpg --dearmor -o /usr/share/keyrings/nvidia-container-toolkit-keyring.gpg \\ \u0026\u0026 curl -s -L https://nvidia.github.io/libnvidia-container/$distribution/libnvidia-container.list | \\ sed 's#deb https://#deb [signed-by=/usr/share/keyrings/nvidia-container-toolkit-keyring.gpg] https://#g' | \\ sudo tee /etc/apt/sources.list.d/nvidia-container-toolkit.list # https://cnvrg.io/how-to-setup-docker-and-nvidia-docker-2-0-on-ubuntu-18-04/ sudo apt-get install -y nvidia-docker2 # inside the docker container (TF) cmake -DSM=86 -DCMAKE_BUILD_TYPE=Release -DBUILD_TF=ON -DTF_PATH=/usr/local/lib/python3.8/dist-packages/tensorflow_core/ -DBUILD_MULTI_GPU=ON .. Unfortunately, no peer access on my private work station. As the other 3 GPUs are connected with PCIe 3.0 x1 slot, which is sharing the DMI to CPU, and far away from GPU0. 1 2 3 4 5 6 7 root@1ba1b6c90879:/workspace/FasterTransformer/build# ./bin/multi_gpu_gpt_example Total ranks: 1. P0 is running with 0 GPU. Device NVIDIA GeForce RTX 3070 [FT][WARNING] Device 0 peer access Device 1 is not available. [FT][WARNING] Device 0 peer access Device 2 is not available. [FT][WARNING] Device 0 peer access Device 3 is not available. Some notes AITemplate: talked with one developer with ","description":"","tags":["transformers","benchmarking"],"title":"Experimenting with Transformers","uri":"/posts/23-04-transformer-benchmarking/"},{"categories":null,"content":"Training and serve a GPT model from scratch (in PyTorch)\nJust a quick summary of this great talk:\nMasked attention: use lower-triangle GeMM to compute masked attention scores (so that only preceding tokens are attended to)\nSparsity: attention is basically a directed graph of nodes representing tokens and edges representing attention scores.\nCross attention: attention where the query comes from a different sequence (i.e., source) than the key and value.\nScaled dot-product attention: 1/sqrt(head_dimension_size) to avoid skewing of output of softmax by ensuring the variance of the input scores is 1. MHA is group of scaled dot-product attention (similar to group convolution).\nPre-training and fine-tuning: pre-training learns the completion rules (unaligned). and fine-tuning is about alignment (supervised learning, reward model from human feedback, RL to optimize sampling policy to please RM)\n","description":"","tags":["mlir","compiler","dataflow"],"title":"Mini-GPT training from scratch","uri":"/posts/23-04-20-gpt-training-from-scratch/"},{"categories":null,"content":"Dataflow in TAPA/HLS/TaskFlow Example in TAPA. A simple example to define a vector adder with two back-to-back kernels in HLS. TAPA’s interface is similar to HLS’s, but with a much++ simple interface for host-accelerator communication. 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 #include \u003ctapa.h\u003e // Kernel specification void Add(tapa::istream\u003cfloat\u003e\u0026 in, tapa::ostream\u003cfloat\u003e\u0026 out, uint64_t n) { for (uint64_t i = 0; i \u003c n; ++i) { out \u003c\u003c (in.read() + 1); } } // DFG specification (top-level function) void Top(tapa::mmap\u003cconst float\u003e in, tapa::mmap\u003cfloat\u003e out, uint64_t n) { tapa::stream\u003cfloat\u003e fifo(\"a\"); tapa::task() .invoke(Add, in, fifo) .invoke(Add, fifo, out) } // Host code: loading bitstream and prepare OCL buffers tapa::invoke(VecAdd, FLAGS_bitstream, tapa::read_only_mmap\u003cconst float\u003e(in), tapa::write_only_mmap\u003cfloat\u003e(out), n); A table to summarize the differences between TAPA, HLS, and TaskFlow. TAPA HLS TaskFlow Dataflow yes yes no (task dependency only) Scheduling data driven data driven work stealing Memory access async/sync mmap* sync mmap sync mmap Targets FPGA FPGA CPU+GPU *async mmap: TAPA inserts an extra AXI adapter that can dynamically batch memory requests into a burst transfer.\nAn MLIR dialect for TAPA and more The dataflow MLIR dialect supports new features from TAPA, including:\nNon-destructive APIs for FIFOs: eot() and peek() Async mmap and sync mmap It also includes new features that are not supported by TAPA, including:\nInput dependent dataflow Explicit task dependency and scheduling Explicit task placement to target devices 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 func @Add(%in: !dataflow.stream\u003cf32\u003e, %out: !dataflow.stream\u003cf32\u003e) { %0 = dataflow.read %in : stream\u003cf32\u003e %1 = addf %0, 1.0 : f32 dataflow.write %1, %out : stream\u003cf32\u003e return } func @Top(%in: memref\u003cf32\u003e, %out: memref\u003cf32\u003e) { %0 = dataflow.mmap_read %in : !dataflow.mmap\u003cf32\u003e %1 = dataflow.mmap_read %out : !dataflow.mmap\u003cf32\u003e %2 = dataflow.stream_create \"fifo0\" : !dataflow.stream\u003cf32\u003e { depth = 32} // Explicit device placement dataflow.task @Add(%0, %2) { device = \"FPGA[0]\" } // Explicit input dependent task dependency dataflow.task @Add(%2, %1) { device = \"FPGA[1]\" } { ^when (%2 : dataflow.stream\u003cf32\u003e): %3 = dataflow.peek %2 : stream\u003cf32\u003e %3 % 2 == 1 } return } Connection to HCL dialect HCL dialect is used for operator-level optimization. All the graph-level optimizations should be done in the dataflow dialect.\nThe HeteroCL python API is untouched. But under the hood, HeteroCL compiler can use dataflow dialect (along with scf, affine, etc) to build the DFG and perform graph-level optimizations (e.g., DFG partition, device placement, operator fusion).\nThe output DFG from dataflow dialect is then passed to HCL dialect for operator-level optimizations, e.g., loop tiling, memory partitioning, and data layout optimizations.\nSimulations We have added support of using multiple MLIR JIT execution engine to simulate task-parallel dataflow programs, but this is only for testing correctness, not performance on hardware.\ndataflow dialect IR can be used to generate TAPA code, and thus we can use the co-sim support in TAPA\nOr alternatively, we can use third-party dataflow simulators to estimate performance of input dataflow programs. For example, DF-Sim or EventQueue\nSome considerations do we need a device or platform dialect to define and manage devices? Progress Dataflow dialect core operations (i.e., stream_create, read, write). A simple translation function that translate dataflow dialect into TAPA code. ","description":"","tags":["mlir","compiler","dataflow"],"title":"MLIR Dataflow Dialect: Design, Simulation, and Optimization","uri":"/posts/23-03-mlir-dataflow-dialect/"},{"categories":null,"content":"Foundation Models Diffusion Model Stable diffusion art: https://stable-diffusion-art.com/how-stable-diffusion-work/\nStable diffusion works on the latent space of a pre-trained VAE (variational auto-encoder). VAE maps the image from pixels to smaller latent space.\nThe UNet (i.e, noise predictor works on the latent space) uses cross-attention’ed with the output from text transformer (i.e., this output is basically a query represented by the human input, and KV is the image knowledge base in UNet).\nHypernetwork and LoRa (and some other fine-tuning techniques) tweak the cross-attention weights to adjust the generated image styles. There are other conditioning like ControlNet to use structure data to guide the noise predictor.\nA hands-on tutorial on how to import these models in webui-stable-diffusion: https://www.youtube.com/watch?v=xkBaR5bIYqc\nFrameworks I will mostly focus on the frameworks for deployment and inference.\nMeta AITemplate The optimization is limited to kernel fusion, and it is heavily relying on CUTLASS (but not other libraries like cuBLAS or cuPARSE).\nThere is no decoder example yet. I haven’t tested the inference speed yet, but one core developer told me it is up to 2x faster than TensorRT.\nTensorRT ","description":"","tags":null,"title":"hecmay’s blog","uri":"/posts/23-04-27-survey-foundation-models/"}]
